{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "You need to setup the TECHIN515 virtual environment to run this lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TECHIN 515: Quantization and Pruning Methods\n",
    "\n",
    "In this lab, we will first go through three (post training) quantization methods: (1) Float-16 Quantization, (2) Dynamic Range Quantization, and (3)Integer Quantization. Then we try out strip pruning for model compression.\n",
    "\n",
    "We will use Efficient Net (`efnet`) ML model as our base ML model and download and use `cats_vs_dogs` dataset for training and testing ML models.\n",
    "\n",
    "Before working on the code, we will need to setup the environment. The following code will display the current version of the tensorflow if you have already installed it in your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will install `tensorflow_datasets` and `tensorflow_model_optimization` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will first download and load `cats_vs_dogs` dataset, load and retrain `efnet` ML model, and experiment with three (post training) quantization methods: (1) Float-16 Quantization, (2) Dynamic Range Quantization, and (3) Integer Quantization.  \n",
    "\n",
    "The following code and material were adapted from the reference [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and packages.\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Preparing the Dataset\n",
    "\n",
    "We can directly import the dataset from the TensorFlow Dataset (tfds). Here we will split the dataset into training, validation, and testing set with a split ratio of 0.7:0.2:0.1. The as_supervised parameter is kept True as we need the labels of the images for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and Loading the CatvsDog dataset.\n",
    "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs', split=['train[:70%]', 'train[70%:90%]', 'train[90%:]'], shuffle_files=True, as_supervised=True, with_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now have a look at the dataset information provided in tfds.info(). The dataset has two classes labeled as ‘cat’ and ‘dog’ with 16283, 4653, 2326 training, validation and testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining dataset information.\n",
    "print(\"Number of  Classes: \" + str(info.features['label'].num_classes))\n",
    "print(\"Classes : \" + str(info.features['label'].names))\n",
    "NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(\"Training Images: \" + str(NUM_TRAIN_IMAGES))\n",
    "NUM_VAL_IMAGES = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "print(\"Validation Images: \" + str(NUM_VAL_IMAGES))\n",
    "NUM_TEST_IMAGES = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "print(\"Testing Images: \" + str(NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function tfds.visualization.show_examples() function displays images and their corresponding labels. It comes very handy when we want to visualize a few images in a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training dataset.import tempfile\n",
    "vis = tfds.visualization.show_examples(train_ds, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen 16 as batch size and 224×224 as image size so that the dataset can be processed effectively and efficiently. To prepare the dataset, the images have been resized accordingly.\n",
    "Let’s also make sure to use buffered prefetching to yield data from the disk. Prefetching overlaps the preprocessing and model execution of a training step. Doing so reduces the step time to the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining batch-size and input image size.\n",
    "batch_size = 16\n",
    "img_size = [224, 224]# Resizing images in the dataset.\n",
    "train_ds_ = train_ds.cache().map(lambda x, y: (tf.image.resize(x, img_size), y)).batch(batch_size).prefetch(buffer_size=10)\n",
    "val_ds_ = val_ds.cache().map(lambda x, y: (tf.image.resize(x, img_size), y)).batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds_ = test_ds.cache().map(lambda x, y: (tf.image.resize(x, img_size), y)).batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed images to the TF Lite model, we need to extract the test images and their labels. We will store them into variables and feed them to TF Lite for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and saving test images and labels from the test dataset.\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for image, label in test_ds_.take(len(test_ds_)).unbatch():\n",
    "    test_images.append(image)\n",
    "    test_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Loading the Model\n",
    "\n",
    "We have chosen the EfiicientNet B0 model pre-trained on the imagenet dataset for image classification purposes. EfficientNet is a state-of-the-art image classification model. It significantly outperforms other ConvNets. \n",
    "\n",
    "Let us import the model form tf.keras.applications().  The last layer has been removed by setting include_top = False .We have set the input image size to 224×224 pixels and kept the pooling layer to be GlobalMaxPooling2D. Let’s load the model and unfreeze all the layers to make them trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model architecture.\n",
    "efnet = tf.keras.applications.EfficientNetB0(include_top = False, weights ='imagenet', input_shape = (224, 224, 3), pooling = 'max')# Unfreezing all the layers of the model.\n",
    "for layer in efnet.layers:\n",
    "    set_trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add a Dense layer to the pre-trained model and train it. This layer will become the last layer, or the inference layer. We will also add Dropout and BatchNormalization to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Dense, BatchNormalization and Dropout layers to the base model.\n",
    "x = Dense(512, activation='relu')(efnet.output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Compiling the Model\n",
    "\n",
    "We are ready to compile the model. We have used Adam Optimizer with an initial learning rate of 0.0001, sparse categorical cross-entropy as the loss function, and accuracy as the metric. Once compiled, we check the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defining the input and output layers of the model.\n",
    "model = Model(inputs=efnet.input, outputs=predictions)\n",
    " \n",
    "# Compiling the model.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = [\"accuracy\"])\n",
    " \n",
    "# Obtaining the model summary.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Model Saving Callback and the Reduce LR Callback.\n",
    "\n",
    "(i) Model Saving Callback saves the model with the best validation accuracy\n",
    "\n",
    "(ii) Reduce LR Callback reduces the learning rate by a factor of 0.1 if validation loss remains the same for three consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining file path of the saved model.\n",
    "filepath = './model.h5'\n",
    " \n",
    "# Defining Model Save Callback and Reduce Learning Rate Callback.\n",
    "model_save = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\",\n",
    "    save_freq=\"epoch\")\n",
    " \n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=1, min_delta=5*1e-3,min_lr =5*1e-9,)\n",
    " \n",
    "callback = [model_save, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "- What is the input size?\n",
    "- Based on model summary, name two examples of layers used in the architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Training the Model\n",
    "\n",
    "The method `model.fit()` is called to train the model. We pass the training and validation datasets and train the model for 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for 15 epochs. \n",
    "# ---> epoch = 3 for testing the code and saving time during the lab\n",
    "# ---> epoch = 15 recommended for better results \n",
    "model.fit(train_ds_, epochs=3, steps_per_epoch=(len(train_ds_)//batch_size), validation_data=val_ds_, validation_steps=(len(val_ds_)//batch_size), shuffle=False, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Evaluating the Model\n",
    "\n",
    "Done training! Let’s check the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the test dataset.\n",
    "_, baseline_model_accuracy = model.evaluate(test_ds_, verbose=0)\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "- Report the model performance when trained for 3 epochs and 15 epochs. \n",
    "- Briefly explain your observation.\n",
    "- If you were to train the model, will you train the model for 3 epochs or 15 epochs? Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Float-16 Quantization\n",
    "In Float-16 quantization, weights are converted to 16-bit floating-point values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the Keras model to the TF Lite Converter.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    " \n",
    "# Using float-16 quantization.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    " \n",
    "# Converting the model.\n",
    "tflite_fp16_model = converter.convert()\n",
    " \n",
    "# Saving the model.\n",
    "with open('./fp_16_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_fp16_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have passed the Float 16 quantization the `converter.target_spec.supported_type` to specify the type of quantization. The rest of the code remains the same for a general way of conversion for the TF Lite Model. In order to get model accuracy, let’s first define evaluate() function that takes in tflite model and returns model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for evaluating TF Lite Model over Test Images\n",
    "def evaluate(interpreter):\n",
    "    prediction= []\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_format = interpreter.get_output_details()[0]['dtype']\n",
    "    \n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 100 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_format)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.tensor(output_index)\n",
    "        predicted_label = np.argmax(output()[0])\n",
    "        prediction.append(predicted_label)\n",
    "    \n",
    "    print('\\n')\n",
    "    # Comparing prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction = np.array(prediction)\n",
    "    accuracy = (prediction == test_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this FP-16 Quantized TF Lite’s model performance on the Test Set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the FP-16 TF Lite model to the interpreter.\n",
    "interpreter = tf.lite.Interpreter('fp_16_model.tflite')\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "# Evaluating the model on the test dataset.\n",
    "test_accuracy = evaluate(interpreter)\n",
    "print('Float 16 Quantized TFLite Model Test Accuracy:', test_accuracy*100)\n",
    "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- Compare the model performance when float 16 quantization is used with the original model performance\n",
    "- Briefly explain your observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Dynamic Range Quantization\n",
    "\n",
    "In Dynamic Range Quantization, weights are converted to 8-bit precision values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the baseline Keras model to the TF Lite Converter.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Using  the Dynamic Range Quantization.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Converting the model\n",
    "tflite_quant_model = converter.convert()\n",
    "# Saving the model.\n",
    "with open('./dynamic_quant_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s evaluate this TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the Dynamic Range Quantized TF Lite model to the Interpreter.\n",
    "interpreter = tf.lite.Interpreter('dynamic_quant_model.tflite') \n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "# Evaluating the model on the test images.\n",
    "test_accuracy = evaluate(interpreter)\n",
    "print('Dynamically  Quantized TFLite Model Test Accuracy:', test_accuracy*100)\n",
    "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- Compare the model performance when dynamic range quantization is used with the original model performance\n",
    "- Briefly explain your observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) Integer Quantization\n",
    "\n",
    "Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers. This resulted in a smaller model and increased inferencing speed, which is valuable for low-power devices such as microcontrollers. \n",
    "\n",
    "The integer quantization requires a representative dataset, i.e. a few images from the training dataset, for the conversion to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the baseline Keras model to the TF Lite Converter.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Defining the representative dataset from training images.\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(test_images).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    " \n",
    "# Using Integer Quantization.\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    " \n",
    "# Setting the input and output tensors to uint8.\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "# Converting the model.\n",
    "int_quant_model = converter.convert()\n",
    " \n",
    "# Saving the Integer Quantized TF Lite model.\n",
    "with open('./int_quant_model.tflite', 'wb') as f:\n",
    "    f.write(int_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s evaluate the obtained Integer Quantized TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the Integer Quantized TF Lite model to the Interpreter.\n",
    "interpreter = tf.lite.Interpreter('./int_quant_model.tflite')\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "# Evaluating the model on the test images.\n",
    "test_accuracy = evaluate(interpreter)\n",
    "print('Integer Quantized TFLite Model Test Accuracy:', test_accuracy*100)\n",
    "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "- Compare the model performance using float 16, dynamic range, and integer quantization\n",
    "- Based on our in-class discussion, explain your observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) Model Pruning \n",
    "\n",
    "You will apply pruning to the whole model and see this in the model summary. In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity. Also note that pruning can be only applied to the dense layers.\n",
    "\n",
    "The following code and material were adapted from the reference [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "#validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = NUM_TRAIN_IMAGES#train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "def apply_pruning_to_dense(layer):\n",
    "    if isinstance(layer, tf.keras.layers.Dense):\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "    return layer\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` \n",
    "# to the layers of the model.\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_pruning_to_dense,\n",
    ")\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "- Experiment with different initial and final sparsity values. Document the model performance under different values\n",
    "- Analyze the pruned model's accuracy and compare it with the baseline and quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Why number of parameters increases after prunning ?\n",
    "\n",
    "\n",
    "Pruning is a model optimization technique aimed at reducing the number of effective parameters by eliminating less important weights. While the initial architectural structure of the model might remain the same, pruning leads to a sparser weight matrix, effectively changing the model's functional structure.\n",
    "\n",
    "In TensorFlow's implementation, pruning introduces non-trainable mask parameters alongside the original weights. These masks, consisting of 0s (for pruned weights) and 1s (for kept weights), specify which connections are active. The model summary will show these non-trainable mask parameters. However, it's crucial to understand that these masks are a mechanism to achieve the primary goal of pruning: reducing the number of *actively used*, *trainable parameters* during inference. This reduction leads to benefits like smaller model size, potentially faster inference, and lower memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and saving test images and labels from the test dataset.\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for image, label in train_ds_.take(len(train_ds_)).unbatch():\n",
    "    train_images.append(image)\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(train_ds_, epochs=3, steps_per_epoch=(len(train_ds_)//batch_size), validation_data=val_ds_, validation_steps=(len(val_ds_)//batch_size), shuffle=False, callbacks=callbacks)\n",
    "# model_for_pruning.fit(train_ds_,\n",
    "#                   batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "#                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(test_ds_, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "- Create a table showing the accuracy and size of each model.\n",
    "- Discuss the trade-offs between accuracy and model size for quantization and pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) Convert the model to ESP32-Compatible Format\n",
    "\n",
    "Our microcontrollers cannot directly load a .tflite file. Instead, we convert it to a C header file. Such conversion generates a C array representation of the model. Run the following command in command line. Then you should see int_quant_esp.h showing up in the directory.\n",
    "\n",
    "`xxd -i int_quant_model.tflite > int_quant_esp.h`\n",
    "\n",
    "Let's now set up ESP32 for TensorFlow Lite. Open Arduino IDE on your laptop, and install ESP32 board manager:\n",
    "\n",
    "`Tools->Board->Boards Manager->ESP32 by Espressif Systems`\n",
    "\n",
    "In library manager, search and install **TensorFlowLite_ESP32**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "### [1] https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning/\n",
    "### [2] https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\n",
    "### [3] https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/comprehensive_guide.ipynb#scrollTo=lvpH1Hg7ULFz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TECHIN515",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}